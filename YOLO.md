# 目标检测
## 目标检测的三大块
- Backbone network主干网络： 是目标检测网络最为核心的部分，大多数时候，backbone选择的好坏，对检测性能影响是巨大的。
- Neck network 主要作用就是将有backbone 输出的特征进行整合
- Detection head 就是若干卷积层进行预测，也有些工作里把head部分称为decoder（解码器） head部分就是再由前面网络输出的特征上去进行预测，约等于是从这些信息里解耦出来图像中物体的类别和位置信息
  
#### Backbone 目标检测网络的主体结构
- 通常，为了实现从图像中检测目标的位置和类别，我们会先从图像中提取出一些必要的特征信息，比如HOG特征，然后利用这些特征去实现定位和分类。如何设计好的Backbone并更换好地从图像中提取信息是至关重要的，因为特征提取不好，自然影响到后续的定位检测
- Backbone这一部分同很长就是将租入VGG ResNet等模型搬过来（去掉最后的global avgpooling和softmax层），这一部分的参数初始化就直接使用在ImageNet上训练好的参数，这一模式就是后来所说的 **ImageNet pretrained** 概念
- 何凯明的《Rethinking ImageNet pretrained》 发表之后，这一概念不是必要的了，一个目标检测模型可以完全随机初始化所有的参数，包括backbone网络的参数，只要训练足够久，也可以达到很高的性能。当然必须训练足够久，而且要精心调参。ImageNet pretrained 仍然是主流思想。
- 常用的backbone模型：
  - VGG网络 VGG16
  - ResNet网络  ResNet50 ResNet101
  - ResNext网络
  - ResNet + DCN
  - DarkNet网络
  - CSPResNet网络
  - MobileNet
  - ShuffleNet

#### Neck 更好地利用网络所提取的特征信息
- backbone网络毕竟是从图像分类任务迁移过来的，其提取特征的模式可能不太适合于detection 因此，在我们最终从这些特征中得到图像中若干目标的类别信息和位置信息之前，有必要对哦他们进行一些处理，在backbone之后，detection head之前，被称为Neck
- 在SSD之前，不论是Faster R-CNN 还是YOLO。他们都只是在backbone输出的最后一层很粗糙的特征图上进行检测。在CNN中，有一个很关键的概念叫做”感受野“，即这一张特征图的pixel能包含原始图像中的多少个像素。直观上来看，backbone最后输出的很粗糙的特征图——通常是stride=32即经过了32倍降采样——具有很大的感受野，这对于大物体来说很友好，但对于小物体而言，过大的感受野容易视角，经过多次采样，小物体的信息也容易倍丢失掉。
- CNN随着网络深度的增加，每一层的特征图所携带的信息量和信息性质不一样——浅层包含的细节信息、轮廓信息、位置信息等更多，深层包含的语义信息更多。
- 常用的Neck模型：
  - FPN
  - RFB
  - ASPP
  - SAM
  - PAN
 
#### detection head 负责检测与定位
- 一张图像在经过了backbone和neck两部分的处理后，就可以准备进行最终的检测了。在这样的特征图上通过添加基层卷积即可进行识别和定位。
- RetinaNet最后的detection head部分就是三条并行的分支，每个分支由四层普通卷积堆叠而成。
- flatten方式会破坏特征的空间结构信息

## YOLOv1  [YOLOv1讲解](https://zhuanlan.zhihu.com/p/364367221)
- 一张图像输入给网络，最后网络输出一个7x7x30的特征图。7x7是原图448x448经过64倍降采样得到的.特征图的每个位置预测两个bbox ，每个bbox包含五个输出参数 置信度C 矩形框参数(c<sub>x</sub> , c<sub>y</sub> , w , h) ，然后一共20个类别（PASCAL VOC 一共20个类别，每个类别的概率值），一共就是30，置信度C的作用是判断此处是否有目标的中心点，即是否有物体。
- 特征图的通道数计算：5B+C 5指的是边界框的置信度和位置参数，B是每个位置预测的bbox数量，C是类别的数量
- YOLOv1是想通过看网格来找到物体的中心点坐标，并确定其类别。每个网格都会输出B个bbox和C个类别的置信度，而每个bbox包含5个参数，因此每个网格都会给出5B+C个预测参数。网格最终输出的预测参数总量就是: S X S X (5B+C)
- YOLOv1就是在每个网格上去做预测，理想情况下，包含了物体中心点的网格会有很高的置信度输出，而不包含中心点的网格的置信度输出应该十分接近0
- YOLOv1一共有三部分输出 分别是objectness class 以及bbox：
  - objectness 就是上面所说的**框的置信度**，用于表征该网格是否有物体；
  - class就是**类别预测**
  - bbox就是**边界框**
  - 前景背景的置信度误差，背景误差需要权重，用以权衡一张图片中前景和背景的权重比，一张图片中背景更多，训练时时就需要提升前景权重用以学习
  - 缺点：每个cell只预测一个类别，如果重叠无法解决重叠物体的检测识别，小物体检测效果一般，长宽比可选但比较单一
  - 优点：快速简单
### 流程
- 构建backbone网络，一般采用ResNet3网络，删掉最后的global avgpool层和softmax层，两者在目标检测任务中不需要
- 搭建Deck网络，可以选择SPP网络，该结构时若干个大小不同的maxpooling堆叠而成
- 搭建Detection head网络，去除掉全连接层，采用多层卷积组成，减小运算参数
- 在后面连接一个预测层，输出的通道为1+C+4 ,其含义分别为objectness预测，class预测和t<sub>x</sub>    t<sub>y</sub>     t<sub>w</sub>     t<sub>h</sub> 
- 在输出结果中分别提取出objectness class 和位置的三部分预测
  - 训练中，将三部分预测传入loss函数中，**计算网络训练过程中的loss**，该部分需要制作正标签样本，计算训练过程中的各个方面的loss值
  - 在测试时，需要将计算出来的x,y和w,h换算出box的左上角坐标和右下角坐标
  - 完成转换后，需要对预测结果进行一次后处理，改处理时为了滤掉得分很低的边界框，滤掉那些针对同一目标的冗余检测，即非极大值抑制（NMS)处理，经过最终的处理后，会得到最终可以输出的三个预测：
    - bbox:包含每一个检测框的x<sub>1</sub> y<sub>1</sub> x<sub>2</sub> y<sub>2</sub>
    - scores：包含每一个检测框的得分
    - cls_inds: 包含每一个检测框的预测类别序号   
### 改进
- 主干网络替换为ResNet50
- Neck网络使用**YOLOF提出的DilatedEncoder**（参考YOLOF)
- detection head是并行的两个分支，最后输出的仍然是三个预测
- 损失函数：objectness人仍然采用MSE损失，class采用交叉熵，边界框采用Glou损失更好，不再采用MSE损失函数
- 在预处理的过程中，保留了图像的长宽比

## YOLOv2
- 引入Batch Normalization加速训练 
  - 该版本舍弃掉DropoutC层，卷积后全部加入Batch Normalization
  - 网络的**每一层的输入**都做了归一化，收敛相对更容易，经过BN处理后的网络会提升2%的mAP
  - 从现在的角度来看，BN已经成为网络必备处理
  - 在YOLOv1中，每一层卷积的结构都是线性卷积和激活函数，并没有使用诸如批归一化(BN),层归一化(LN),实列归一化(IN),卷积层的组成从原先的线性卷积与激活函数的组合改进为后来常用的“卷积三件套”：线性卷积，BN层以及激活函数
- 采用更大的分辨率
  - V1训练时用的是224*224， 测试时使用的是448*448
  - 可能导致模型水土不服，V2训练时在224*224的低分辨率图像上训练好的分类网络的基础上额外又进行了10次448*448的高分辨率图像微调
  - 使用高分辨率分类器后，V2的mAP提升了4%
- 废除了全连接层，经过五次降采样，最终输出为13*13（DarkNet网络)，采用了1X1卷积，节省了许多参数
  - 全连接层不仅破坏了先前的特征图所包含的空间信息结构，同时也导致参数量爆炸。
- **anchor boxes 先验框**
  - 通过引入anchor boxes ，使得预测的box数量更多，网络值需要学习将先验框映射到真实框的尺寸的偏移量即可，无需再学习整个真实框的尺寸信息，使得训练变得容易
  - 之前每个网格处会有1个边界框输出，而现在变成了预测k个先验框的偏移量
  - 加入先验框之后，每一个先验框都将预测一个类别和置信度，即每个网格处会有多个边界框的预测输出。每个边界框的预测输出包含一个置信度，四个边界框的位置参数和类别预测
  - 跟faster rcnn系列不同的是先验框并不是直接按照长宽固定比给定
  - 每个网格输出多个检测结果有助于网络检测更多的物体，提升了召回率，但是精度没有更大提升
- **Kmeans聚类先验框**
  - 通过聚类的方法所获取的先验框显然会适用于所使用的数据集，但是A数据集聚类出来的先验框显然难以适应新的B数据集，更换数据集时将需要重新进行聚类，样本不够充分，聚类出来的效果也不会太好
  - 由聚类所获得先验框严重依赖于数据集本身，倘若数据集规模过小，样本不够丰富，由聚类得到的先验框也未必会提供很好的尺寸信息
- 新的主干网络:设计了新的backbone网络取代原先的GoogleNet风格的backbone，新网络被命名为DarkNet19，该网络一共包含19个卷积层
- 感受野：特征图上的点能够看到原始图像的多大区域
  - 堆叠小的卷积核所需的参数更少一些，并且卷积过程越多，特征提取也会越细致，加入的非线性变换也随着增多，还不会增大权重参数个数，这就是VGG网络的基本出发点，用小的卷积核来完成特征提取操作
- 特征融合
  - 将backbone的17层卷积输出的特征图26\*26\*512拿出来做一次**特殊的降采样**，得到一个13*13*2048特征图，然后将改特征图与网络输出的13*13*1024的特征图进行通道的维度上进行拼接，在这张融合了更多信息的特征图上进行检测
- 多尺度训练
  - 一个十分常见的图像处理操作是图像金字塔，即将一张图像缩放到不同的尺寸，同一目的，在不同分辨率的图像中，所包含的信息量也是不一样的。网络能够在不同尺寸下去感知同一目标，从而增强其本身对目标尺寸变化的鲁棒性
- 缺点：小目标检测性能较差，只是用了最后一个经过32倍降采样的特征图，尽管YOLOv2使用立刻passthrough技术将16倍降采样的特征图融合到了32倍降采样的特征图中，但最终检测仍然是在经过32倍降采样的特征图上进行，为了解决改问题，后续的YOLOv3引入了FPN机制，融合不同尺度特征图中的位置信息和语义信息

## YOLOv3
- 更换上更好的backbone网络，DarkNet53,该网络相比DarkNet19具有更多的卷积层，同时添加了残差网络中的残差连接结构，提升网络性能。同时该网络中的降采样操作没有使用Maxpooling层，而是采用卷积层使用步长为2来实现。
- yolov3往往需要在COCO上训练超过200个epoch，并且使用包括随机水平翻转，颜色扰动，随机裁剪，和多尺寸训练在内等大量的数据增强手段。但其性能强，速度快，计算量较小。RetinaNet训练少量epoch，同时只需要简单的数据增强即可
- 使用FPN与多级操作
  - 网络浅层的特征图包含更多的细节信息，但语义信息较少，深层的特征图与之相反。卷积神经网络的降采样操作对小目标的损害显著大于大目标。
  - 随着网络深度的加深，降采样操作的增多，细节信息不断被破坏，致使小物体的检测效果越来越差，大目标由于像素较多，仅靠网络的前几层还不足以使得网络能够认识到大物体(感受野不充分)，但随着层数变多，感受野逐渐增大，网络对于大目标的检测认识越来越充分
  - 解决方案：浅层网络负责检测较小的目标，深层网络负责检测较大的目标。将深层网络的语义信息融合到浅层网络中去
  - FPN基本思想：对深层网络输出的特征图使用上采样操作，然后与浅层网络进行融合，使得来自于不同尺度的细节信息和语义信息得到了有效融合
  - 多级检测：不同尺度的物体由不同尺度的特征图去做检测，不像YOLOv2那样，都堆在最后的C5特征图上做检测。FPN在该基础上，让不同尺度的特征图先融合一遍，再做检测。
- **大多数情况下，分类网络作为目标检测网络的backbone时，往往时不需要最后的全局池化层和分类层！**
- softmax改进，进行多标签预测

## YOLOV4
- backbone: CSPDarknet53
  - 增强CNN的学习能力
  - 减少显存利用
  - 移除计算瓶颈
- Neck:SPP PAN 原文中的融合方式，即特征层之间融合时直接通过相加的方式进行融合，YOLOV4是**在通道方向concat拼接的方式**进行融合的
- head：同YOLO-v3不变

### SPP net
- 将特征层分别通过一个池化核大小为 5\*5， 9\*9，13\*13的最大池化层，然后在通道方向进行concat拼接再做进一步融合，这样能在一定程度上解决多目标多尺度的问题
### PAN net
- 在FPN(从顶到底信息融合)的基础上加上了从低到顶的信息融合，
- 
## RCNN
- 流程步骤：
  - 一张图像生成1-2k个候选区域(使用Selective Search方法)
  - 对每个候选区域，使用深度网络提取特征，将2000候选区域缩放到227\*227，接着将候选区域输入事先训练好的CNN网络，获取4096维的特征，得到2000*4096维矩阵
  - 特征送入每一类的SVM分类器，判别该类是否属于该类
    - 将2000\*4096维特征与20个SVM组成的权值矩阵4096\*20相乘，，获得2000\*20维矩阵表示每个建议框是某个目标的得分，分别对上述2000*20维矩阵中每一列即每一类进行非极大值抑制剔除重叠建议框，得到该列即该类中得分最高的一些建议框
  - 使用回归器精细修正候选框位置 
    - 对NMS处理后剩余的建议框进一步筛选，接着分别用20个回归器对上述20个类别中剩余的建议框进行回归操作，最终得到每个类别修正后的得分最高的bounding box。
## Fast RCNN
- 流程步骤：
  - 一张图像生成1-2k个候选区域(使用Selective Search方法)
  - 将图像输入网络得到相应的特征图，将SS算法生成的候选框投影到特征图上获得相应的特征矩阵
    - Fast-RCNN将整张图像送入网络，紧接着从特征图像上提取相应的候选区域，这些候选区域的特征不需要再重复计算
  - 将每个特征矩阵通过ROI(Region of Interest) pooling层缩放到7\*7大小的特征图，接着将特征图展平通过一系列全连接层得到预测结果 
  - 分类器：输出N+1个类别的概率(N为检测目标的种类，1为背景)共N+1个节点
  - 边界框回归器：输出对应N+1个类别的候选边界框回归参数(d<sub/>x d<sub/>y d<sub/>w d<sub/>h)共(N+1)*4个节点
## Faster RCNN  (RPN+Fast RCNN)
- 流程步骤：
  - 将图像输入网络得到相应的特征图
  - 使用RPN结构生成候选框，将RPN生成的候选框投影到特征图上获得相应的特征矩阵
  - 将每个特征矩阵通过ROI pooling层缩放到7\*7大小的特征图，接着将特征图展平通过一系列全连接层得到预测结果


# 寻找创新点
- 数据集的改动：
  - 噪声，几何变换，遮挡，光照条件，场景依赖
- 模型的问题：
  - 模型体积，推理速度，收敛困难，非端到端，后处理优化
- 结构替换：
  - transform, FCN, AE
- 特定场景的应用：
  - 通用模型考虑泛化能力 特定应用考虑专用性 夜间检测 水下检测 鱼眼相机检测
- 本研究方向的继承性创新点（自然演进）
- 其他方向的既有方法（嫁接到其他任务）
- 细节上的创新（数据增强、数据集，损失函数的设计）
- coding/实验
  - 找到baseline论文的代码
  - 在baseline代码上实现期望功能的最小化实现
  - 逐步实现最终的功能代码，同时做实现验证各部分设计的效果