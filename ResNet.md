### ResNet
- ResNes是一种残差网络，咱们可以理解为一个子网络，这个子网络经过堆叠可以构成一个很深的网络。
- 我们知道，网络越深，咱们获取的信息越多，而且特征信息也越丰富。但是根据实验表明，随着网络的加深，优化效果反而越差，测试数据和训练数据的准确率反而降低了，这是由于**网络的加深会造成梯度爆炸和梯度消失的问题**，针对该问题的解决方式有：对输入数据和中间层的数据进行归一化操作，这种方法可以保证网络在反向传播中采用随机梯度下降（SGD)，从而让网络达到收敛。但是该方法只对几十层的网络有用，随着网络层数加深，这种方法就无用武之处了，会出现退化问题。
  - 梯度消失：若每一层的误差梯度小于1，反向传播时，网络越深，梯度越趋近于0
  - 梯度爆炸：若每一层的误差梯度大于1，反向传播时，网络越深，梯度越来越大
  - CNN参数个数 = 卷积核尺寸 X 卷积核深度 X 卷积核组数 = 卷积核尺寸 X 输入特征矩阵深度 X 输出特征矩阵深度 
  - 搭建深层次网络时，采用三层的残差结构
- ResNet在一定程度上解决了上面问题，通过直接将输入信息绕道传到输出，保护信息的完整性，整个网络只需要学习输入输出差别的那一部分，简化了学习目标和难度
- 使用Batch Normalization加速训练（丢弃dropout）
  
### 网络深度问题
- 因为CNN能够提取low/mid/high-level的特征，网络的梣属越多，意味着能够提取到的不同level的特征越丰富，并且越深的网络提取的特征越抽象，越具有语义信息
- 对于原来的网络简单地增加深度会导致梯度弥散或梯度爆炸
- 正则初始化会出现另一个问题，退化问题。退化问题可能是因为深层的网络并不是那么好训练，也就是求解器很难去利用多层网络拟合同等函数。

### 残差网络
- 残差的思想是去掉相同的主体部分，从而突出微小的变化。加深的残差网络能够比简单叠加层生产的深度网络更容易优化.同等映射函数肯不会那么好优化，但是对于残差学习，求解器根据输入的同等映射，更容易发生扰动，比直接学习一个同等映射函数要容易得多。
- 设计网络的规则
  - 对于输出feature map大小相同的层，有相同数量的filters，即channels数相同
  - 当feature map大小减半时（池化），filters数量翻倍
  - 对于残差网络，维度匹配的shortcut连接为实现，反之为虚线，维度不匹配时，同等映射的可选方案：
    - 直接通过zero padding 来增加维度(channel)
    - 乘以W矩阵投影到新的空间。利用1*1卷积实现，直接改变卷积的filters数目，这种会增加参数。
    - 投影法会比zero padding表现稍好一些。因为zero padding的部分没有参与残差学习。实验表明，将维度匹配或不匹配的同等映射全用投影法会取得更稍好的结果，但是考虑到不增加复杂度和参数，不采用该方法
  - 虚线残差结构 具有调整输入特征矩阵shape的使命（将特征矩阵的高和宽缩减为原来的一半，将深度channel调整成下一层残差结构所需要的channel）
  
### BN
- 我们在图像预处理过程中通常会对图像进行标准化处理，这样能够加速网络的收敛。满足某一个分布规律不单单指一个feature map 的数据要满足分布规律，理论上是指整个训练样本集所对应feature map的数据要满足分布规律，BN的目的就是时feature map满足均值为0，方差为1的分布规律。

### 迁移学习
- 载入权重后训练所有参数
- 载入权重后只训练最后几层参数
- 载入权重后在原网络基础上再添加一层全连接层，仅训练最后一个全连接层

的速度古
### 各种卷积的参数量与计算量
- 常规卷积的参数量
  - 输入特征矩阵 H X W X C<sub>in</sub>  , 卷积核尺寸 K X K X C<sub>out</sub>  , 输出特征矩阵 H X W X C<sub>out</sub> 
  - 参数量 K X K X C<sub>in</sub>  X C<sub>out</sub>   计算量 H X W X K X K X C<sub>in</sub>  X C<sub>out</sub> 
  - 参数量与输入特征矩阵的高宽无关，而计算量与输入特诊矩阵与卷积核尺寸和个数均有关
- 分组卷积 在常规卷积的基础上，按输入通道数进行分组，假设分为G组
  -  输入特征矩阵 H X W X C<sub>in</sub>  , 卷积核尺寸 K X K X C<sub>out</sub>  , 输出特征矩阵 H X W X C<sub>out</sub> 
  -  参数量 K X K X C<sub>in</sub>  X C<sub>out</sub>  X V 计算量 H X W X K X K X C<sub>in</sub>  X C<sub>out</sub> > X 1/G
  -  分组卷积的参数量与计算量均在常规卷积的基础上乘上1/G
-  深度可分离卷积 DepthWise卷积+PointWise卷积
   -  参数量 [各种卷积的参数量和计算量计算](https://blog.csdn.net/kangdi7547/article/details/117925389)
   -  [深度可分离卷积详细分析](https://blog.csdn.net/qq_39987519/article/details/107177190) 