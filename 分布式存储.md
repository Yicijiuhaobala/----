# 分布式系统
- 一群独立计算机集合共同对外提供服务，但是对于系统的用户来说，就像是一台计算机在提供服务。分布式意味着可以采用更多普通的计算机组成分布式集群对外提供服务。
- 特性
  - 分布性
  - 对等性 每一个节点都是独立的 并无主从之分
  - 自治性
  - 并发性

## 分布式理论基础
- CAP理论
  - C代表一致性 Consistency  一个写操作返回成功，那么之后的读请求都必须读到这个新数据；如果返回失败，那么所有读操作都将不能读到这个数据。所有节点访问同一份最新的数据
  - A代表可用性 Availability  对数据更新具备高可用性，请求能够即及时处理，不会一直等待，即使出现节点失效
  - P代表分区容错性 Partition tolerance  能容忍网络分区，在网络断开的时候，被分隔的节点仍能对外提供服务
  - 理解CAP理论最简单的方式是想象两个副本处于分区两侧，即两个副本之间的网络断开，不能通信。
    - 如果允许其中一个副本更新，则会导致数据不一致，即丧失了C性质。
    - 如果为了保证一致性，将分区某一侧的副本设置为不可用，那么又丧失了A性质。
    - 除非两个副本可以互相通信，才能既保证C又保证A，这又会导致丧失P性质。
- BASE理论 
  - Basically Available（基本可用）分布式系统在出现不可预知故障的时候，允许损失部分可用性
  - Soft state（软状态）软状态也称为弱状态，和硬状态相对，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。
  - Eventually consistent（最终一致性）最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性
  - BASE理论是对CAP中一致性和可用性权衡的结果。其核心思想是即使无法做到强一致性，也可以通过业务，牺牲强一致性而获得可用性，并允许数据在一段时间内是不一致的，但是最终达到一致性状态
  
## 分布式一致性算法
- 一致性算法的目的是保证在分布式系统中，多数据副本节点数据一致性。
- 一致性Hash算法
  - 
- Paxos算法
- Raft算法
- ZAB算法
- Snowflake算法

## 分布式系统 全局唯一ID实现方案
- 特点  **全局不重复 不可猜测且呈递增态势**
  - 全局唯一性
  - 趋势递增
  - 单调递增
  - 信息安全
- 分类：
  - 类DB型 根据设置不同起始值和步长来实现趋势递增，需要考虑服务的容错性和可用性
  - 类雪花算法型 将64位划分为不同的段，每段代表不同的涵义，基本就是**时间戳、机器ID和序列数**。这种方案就是需要考虑时钟回拨的问题以及做一些 buffer的缓冲设计提高性能。而且**可通过将三者（时间戳，机器ID，序列数）划分不同的位数来改变使用寿命和并发数。**
- 在分布式服务架构下分库分表的设计，使得多个库和多个表存储相同的业务数据，自增Id就会产生相同的ID，不能保证主键的唯一性
- UUID： 通用唯一识别码的缩写  基于时间的，DCE安全的，基于名字的(MD5)，随机UUID、基于名字的UUID(SHA1)
  - 缺点： 不易于存储，UUID太长，基于MAC地址生成的UUID可能暴露安全信息，对于Mysql蓑衣不利，UUID的无序性可能会引起数据位置频繁变动，影响数据库性能
- 数据库自身实现：将分布式系统中数据库的同一个业务表的自增ID设计成不一样的起始值，然后设置固定的补偿，步长的值为分库的数量或者分表的数量  ID发号强依赖DB ，也存在重复发号的可能性
- Redis实现： 通过向INCR INCRBY这样的自增原子命令，利用redis自身的单线程和特点来保证生成的ID唯一有序 。它的性能是比较高的，且生成的数据是有序的，对排序业务有利
- 雪花算法：雪花算法生成的Id是趋势递增，不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成的ID性能也是非常高，可以根据自身业务特性分配Bit位  **雪花算法是强依赖于机器时钟，一旦机器时钟发生回拨，都是可能导致重复发好或者服务不可用状态**
- 百度 UidGenerator
- 美团 Leaf算法
- Mist 薄雾算法

## 分布式系统 分布式锁及实现方案
- 线程锁： 主要是用来给方法、代码块加锁。**线程锁只在同一个JVM中有效**，线程锁的实现根本上是在依靠线程之间共享内存实现的
- 进程锁：为了控制同一个操作系统中多个进程访问某个共享资源，因为进程具有独立性，各个进程无法访问其他进程的资源，无法通过线程锁实现进程锁
- 分布式锁：**多个进程不在同一个系统中，使用分布式锁控制多个进程对资源的访问**
- 分布式锁的要求：
  - 互斥：在任何时候只有一个客户端可以持有锁
  - 无死锁：即使锁定资源的客户端崩溃或者被分区也是可以获取到锁，通过超时机制实现
  - 容错性：只要大多数redis节点都启动，客户端都可以获取到锁或者释放锁
  - 同源性：A获取的锁不能由B来释放
  - 非阻塞：如果获取不到锁不会无限等待
  - 高性能：加锁解锁是高性能的
- 实现方案
  - 基于数据库实现分布式锁
    - **基于数据库表（锁表**） 直接创建一张锁表，然后通过操作该表中的数据来实现。当我们想要获取锁的时候，就在该表中增加一条记录，想要释放锁就删除该条数据
    - **基于悲观锁**   **如果一条SQL语句用不到索引就不会使用行级锁，会使用表级锁把整张表锁住**
      - 在对任意记录进行修改前，先尝试为该记录加上排他锁（exclusive locking）。
      - 如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常。 具体响应方式由开发者根据实际需要决定。
      - 如果成功加锁，那么就可以对记录做修改，事务完成后就会解锁了。
      - 其间如果有其他对该记录做修改或加排他锁的操作，都会等待我们解锁或直接抛出异常。
    - **基于乐观锁** 
      - 乐观锁假设多用户并发的事务在处理时不会彼此相互影响，各事务能够在不产生锁的情况下处理各自影响的那部分数据。在提交数据更新之前，每个事务都会先检查在该事务读取数据后，有没有其他事务有修改了该数据。如果其他事务有更新的话，正在提交的事务会进行回滚。
    - **基于Redis实现分布式锁**  **缺陷：当获取锁被拒绝之后，需要不断的循环，重新发送获取锁的请求，直到请求成功，这就造成空转，浪费宝贵的时间**
      - **set NX PX + lua**   加锁： set NX PX + 重试+重试间隔  解锁：采用lua脚本 在删除key之前，一定要判断服务A持有的value于Redis内存储的value是否一致，贸然使用服务A持有的key来删除锁，可能误将服务B的锁释放掉
      - **基于RedLock实现分布式锁**  
        - 假设有两个服务A B都希望获取锁 有一个包含了5个redis master的redis集群
        - 客户端获取当前时间戳
        - 服务A轮询每个master节点，尝试创建锁。锁的过期时间比较短，redlock算法会尝试在大多数节点上分别创建锁，加入节点总数为n，那么大多数节点为n/2+1
        - 客户端计算成功建立完锁的时间，如果建锁时间小于超时时间，则可以判定锁创建成功，反之锁创建失败，则依次遍历节点删除锁
        - 只要有其他服务创建过分布式锁，那么当前服务就必须轮询尝试获取锁
      - **基于Redis客户端** （封装了前面两种实现分布式锁的方式）
        - 特点
          - redission所有指令都通过lua脚本执行，保证了操作的原子性
          - redission设置了watchdog看门狗，保证了没有死锁发生
          - redission支持redlock的实现方式
        - 过程
          - 线程去获取锁，获取成功：执行lua脚本，保存数据到redis数据库
          - 线程去获取锁，获取失败：订阅了解锁消息，然后尝试获取锁，获取成功后，执行lua脚本，保存数据到redis数据库
        - watchdog自动延时机制
          - 客户端一旦加锁成功，就会启动一个watchdog看门狗，它是一个后台进程，会每隔10秒检查一下，如果客户端还持有锁Key，那么就会不断地延长key的生存时间
        - 可重入： 每次lock会调用incrby， 每次unlock都会减一 
    - **基于zookeeper实现分布式锁**
      - 顺序节点：创建一个用于发号的节点，然后以它为父亲节点以此发号
      - 获得最小号节点得锁：由于序号的递增行，可以规定排号最小的那个节点获取锁。所以，每个线程在尝试占用锁之前，首先判断自己的排号是不是当前最小，如果是，则获取到锁
      - 节点监听机制：每个节点抢占锁之前，先抢号创建自己的ZNode，同样释放锁的时候，就需要删除抢号的ZNode。抢号成功后，如果不是排号最小的节点，就处于等待前一个ZNode通知的状态。当前一个ZNode删除的时候，就轮到自己占有锁的时候了、以此往后传递通知

## 分布式系统 分布式事务及实现方案



## 分布式系统 分布式缓存及实现方案
- 本地缓存：应用和缓存在一个进程里面，请求缓存非常快速，但缓存与应用耦合，多个程序无法直接的共享缓存，每个节点需要单独地维护自己的缓存
- 分布式缓存：与应用分离的缓存组件或者服务，与本地应用隔离，多个应用可以直接的共享缓存
- memcached分布式缓存
- Redis缓存

## 分布式系统 分布式任务及实现方案



## 分布式系统 分布式会话及实现方案
- 单机 session+cookie
- 多机器
  - 在负载均衡侧 session粘滞 即将同一个客户端的请求都转发到同一台服务器上，需要使用到负载均衡策略，一旦该服务器宕机 对应的客户端将无法访问请求
  - session数据同步  所有的服务器都同步session数据，开销比较大
- **多机器 集群 session集群管理**  比如使用redis  不把session存在在每一台服务器上，单独用一台服务器存储所有的session数据，进行集中管理。不需要进行session数据的同步
- **无状态token** JWT 在服务端不需要存储用户的登陆记录
  - 客户端通过用户名和密码登陆服务器
  - 服务端对客户端身份进行认证
  - 服务端对该用户生成token 返回给客户端
  - 客户端将token保存在本地浏览器，一般保存在cookie中
  - 客户端发起请求需要携带token
  - 服务端收到请求后，首先验证token之后再返回数据










### 哈希取余分区（不适用于大厂）
- 两亿条记录就是两亿个k-v 我们单机无法进行存储 必须使用分布式多机存储 假设有3台机器构成一个集群，用户每次读写操作都是根据公式：hash(key)%N个机器台数，计算出哈希值，用来决定数据映射到哪个节点上
  - 优点：简单粗暴，直接有效。使用Hash算法让固定的一部分请求落在同一台服务器上，这样每台服务器固定处理一部分请求（并维护这些请求的数据），起到负载均衡和分而治之的作用
  - 缺点：进行扩容和缩容比较麻烦，某个机器宕机了，由于台数数量发生变化，会导致hash取余全部数据重新洗牌

### 一致性哈希算法
- 提出一致性Hash解决方案，目的是当服务器个数发生变动时，尽量减少影响客户端到服务器的映射关系
- 一致性哈希算法将整个哈希值空间组织成一个虚拟的圆环
- 将集群中各个服务器IP节点映射到环上的某一个位置，根绝函数Hash计算出IP节点对应的哈希值
- 当我们需要储存一个kv键值对时，首先计算key的hash值，将这个key使用相同的函数Hash计算出哈希值并确定此数据在环上的位置，从此位置沿着环顺时针“行走”，第一台遇到的服务器就是其应该定位的服务器，并将该键值对存储在该节点上
  - 优点：容错性，扩展性 加入和删除节点，只会影响哈希环中顺时针方向的相邻节点，对其他节点无影响。
  - 缺点：一致性Hash算法在服务节点太少时，容易因为节点分布不均匀而造成数据倾斜（被缓存的对象大部分集中缓存在某一台服务器上）问题。

### 哈希槽分区
- 解决均匀分配的问题。在数据和节点之间又加入了一层，把这一层称为哈希槽，用于管理数据和节点之间的关系，现在就相当于节点上放的时槽，槽里放的是数据
- 哈希解决的是映射问题，使用key的哈希值来计算所在的槽，便于数据分配。

